{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors\n",
        "-> They are specialized data structre similar to arrays and metrices. In Pytorch they are used to encode the input and the output of a model as well as the model parameters.\n",
        "\n",
        "-> Tensors are similar to NumPy's ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing."
      ],
      "metadata": {
        "id": "itIAmJFTylno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LjQiauMUyoVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Initialization"
      ],
      "metadata": {
        "id": "rYOqIpbI0AXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Directly from data"
      ],
      "metadata": {
        "id": "kJPn2NLW0Fdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1, 2.0], [3, 4]]\n",
        "x_data = torch.tensor(data)"
      ],
      "metadata": {
        "id": "vZ2qHfsOz6GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### From a NumPy array\n",
        "Tensors can be created from NumPy arrays and vice versa"
      ],
      "metadata": {
        "id": "s6mHBJxI0R3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ],
      "metadata": {
        "id": "4rLrhz1l0TM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####From another tensor:\n",
        "\n",
        "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden."
      ],
      "metadata": {
        "id": "EtOec4Sv0oMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mnPZRcm0eoB",
        "outputId": "c60fc3ed-0e49-4ae7-b3ad-e38123d1d08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1., 1.],\n",
            "        [1., 1.]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.8867, 0.2708],\n",
            "        [0.9708, 0.8145]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####With random or constant values:\n",
        "\n",
        "`shape` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gxi3OJM00xz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (2, 3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O47pMb5-1QDJ",
        "outputId": "4a4c5eba-e4b1-46e1-dcb1-346e01ba7e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.5192, 0.7447, 0.4648],\n",
            "        [0.6184, 0.0445, 0.1417]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tensor Attributes\n",
        "Tensor attributes describe their shape, datatype, and the device on which they are stored."
      ],
      "metadata": {
        "id": "qUnN5ESj1VrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(3, 4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBr9BTxW1hEI",
        "outputId": "89e9b99b-f9cb-4f7c-f2db-fa08bce1e203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tensor Operations\n",
        "Over 100 tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random sampling, and more are available"
      ],
      "metadata": {
        "id": "Q7qKcwtY1xeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to('cuda')\n",
        "  print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "id": "YQdUDLbJ13P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Standard numpy-like indexing and slicing:\n",
        "\n"
      ],
      "metadata": {
        "id": "DnrDxDfb2IOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPFBrnFL175U",
        "outputId": "3bf73ed7-4a07-49fe-fc9e-52f43bbe6a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Joining tensors** We can use `torch.cat` or `torch.stack` to concatenate a sequence of tensors along a given dimension.\n",
        "\n",
        "\n",
        "This is one of the most common confusing topics when learning tensor operations in PyTorch (or NumPy).\n",
        "\n",
        "Let's break it down conceptually first, then use diagrams, and finally compare it with `cat`.\n",
        "\n",
        "Here is the golden rule to remember:\n",
        "\n",
        "> **`torch.stack` creates a NEW dimension.**\n",
        "> It takes  tensors of the *exact same shape* and packages them together into a new, higher-dimensional tensor.\n",
        "\n",
        "Let's look at your input tensor `x`. It is a 2D tensor with a shape of `(2, 3)`.\n",
        "\n",
        "```\n",
        "x shape: (2 rows, 3 columns)\n",
        "\n",
        "      Col 0    Col 1    Col 2\n",
        "Row 0 [ 0.33,    0.12,    0.23]\n",
        "Row 1 [ 0.23,   -1.12,   -0.18]\n",
        "\n",
        "```\n",
        "\n",
        "We can visualize `x` as a single sheet of paper with a 2x3 grid written on it.\n",
        "\n",
        "---\n",
        "\n",
        "### Visualizing `torch.stack((x, x), dim=...)`\n",
        "\n",
        "We are stacking two identical copies of `x`. Let's call them **Sheet A** and **Sheet B**.\n",
        "\n",
        "#### 1. `torch.stack((x, x), dim=0)`\n",
        "\n",
        "* **Concept:** Create a NEW dimension at index 0 (the very beginning).\n",
        "* **Action:** We take the entire Sheet A and the entire Sheet B and treat them as items in a new list.\n",
        "* **Analogy:** Put Sheet A on a table. Put Sheet B directly on top of it. You now have a stack of sheets.\n",
        "* **Resulting Shape:** The original shape was `(2, 3)`. The new dimension (size 2, because we have 2 sheets) is inserted at the front. Result: `(2, 2, 3)`.\n",
        "\n",
        "**Diagram:**\n",
        "\n",
        "```text\n",
        "New Dimension 0 (Depth/Batch)\n",
        "       |\n",
        "       |  [Sheet A: (2x3)]\n",
        "       |  ---------------------\n",
        "       |  | 0.33,  0.12,  0.23 |  <- Row 0\n",
        "idx 0->|  | 0.23, -1.12, -0.18 |  <- Row 1\n",
        "       |  ---------------------\n",
        "       |\n",
        "       |  [Sheet B: (2x3)]\n",
        "       |  ---------------------\n",
        "       |  | 0.33,  0.12,  0.23 |  <- Row 0\n",
        "idx 1->|  | 0.23, -1.12, -0.18 |  <- Row 1\n",
        "       |  ---------------------\n",
        "\n",
        "```\n",
        "\n",
        "*Look at the output in your code for dim=0. The outermost brackets contain two blocks. The first block is Sheet A, the second is Sheet B.*\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. `torch.stack((x, x), dim=1)`\n",
        "\n",
        "* **Concept:** Create a NEW dimension at index 1 (between rows and columns).\n",
        "* **Action:** We go inside the existing Row dimension (dim 0). For *each row*, we create a stack of the corresponding rows from Sheet A and Sheet B.\n",
        "* **Analogy:**\n",
        "1. Cut Row 0 out of Sheet A. Cut Row 0 out of Sheet B. Stack these two strips together.\n",
        "2. Cut Row 1 out of Sheet A. Cut Row 1 out of Sheet B. Stack these two strips together.\n",
        "3. Put the resulting two stacks next to each other.\n",
        "\n",
        "\n",
        "* **Resulting Shape:** Original `(2, 3)`. Insert new dim (size 2) at index 1. Result: `(2, 2, 3)`.\n",
        "\n",
        "**Diagram:**\n",
        "\n",
        "```text\n",
        "Overall structure remains 2 main groups (originally rows).\n",
        "Inside each group, we now have a stack of 2.\n",
        "\n",
        "Group 0 (Original Row 0s):\n",
        "   NEW Dim 1\n",
        "      |\n",
        "idx 0 |  [ 0.33,  0.12,  0.23 ]  (from Sheet A)\n",
        "idx 1 |  [ 0.33,  0.12,  0.23 ]  (from Sheet B)\n",
        "\n",
        "Group 1 (Original Row 1s):\n",
        "   NEW Dim 1\n",
        "      |\n",
        "idx 0 |  [ 0.23, -1.12, -0.18 ]  (from Sheet A)\n",
        "idx 1 |  [ 0.23, -1.12, -0.18 ]  (from Sheet B)\n",
        "\n",
        "```\n",
        "\n",
        "*Look at your code output for dim=1. You see two main blocks. The first block contains Row 0 of x followed by Row 0 of x. The second block contains Row 1 of x followed by Row 1 of x.*\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. `torch.stack((x, x), dim=2)` (same as `dim=-1`)\n",
        "\n",
        "* **Concept:** Create a NEW dimension at index 2 (the very end, the deepest level).\n",
        "* **Action:** Go down to the individual numbers. At every position (e.g., row 0, col 0), take the number from Sheet A and the number from Sheet B and stack them into a pair.\n",
        "* **Analogy:** Place Sheet A flat. Place Sheet B directly *behind* it (like looking through transparent layers). For every cell in the grid, you now see two numbers deep.\n",
        "* **Resulting Shape:** Original `(2, 3)`. Insert new dim (size 2) at the end. Result: `(2, 3, 2)`.\n",
        "\n",
        "**Diagram:**\n",
        "\n",
        "```text\n",
        "The structure looks like the original (2 rows, 3 cols),\n",
        "but every single number has been replaced by a stack of 2.\n",
        "\n",
        "Row 0:\n",
        "Col0 stack: [0.33 (A), 0.33 (B)]\n",
        "Col1 stack: [0.12 (A), 0.12 (B)]\n",
        "Col2 stack: [0.23 (A), 0.23 (B)]\n",
        "\n",
        "Row 1:\n",
        "Col0 stack: [0.23 (A), 0.23 (B)]\n",
        "Col1 stack: [-1.12 (A), -1.12 (B)]\n",
        "Col2 stack: [-0.18 (A), -0.18 (B)]\n",
        "\n",
        "```\n",
        "\n",
        "*Look at your code output for dim=2. It looks like a 2x3 grid, but every entry is now a little pair, like `[0.3367, 0.3367]`.*\n",
        "\n",
        "---\n",
        "\n",
        "### The Difference Between `torch.stack` and `torch.cat`\n",
        "\n",
        "This is crucial. They both combine tensors, but they do it differently.\n",
        "\n",
        "1. **`torch.stack`**: Creates a **NEW** dimension. The inputs must have the *exact same shape*.\n",
        "2. **`torch.cat` (concatenate)**: Joins tensors along an **EXISTING** dimension. The inputs must have the same shape *except* on the dimension you are joining along.\n",
        "\n",
        "#### Analogy:\n",
        "\n",
        "* **Stacking:** You have two thin paperback books. You put one on top of the other. You now have a pile of books (a new \"height\" dimension has been created).\n",
        "* **Concatenating:** You have two thin paperback books. You tape the last page of the first book to the first page of the second book. You still have one book, just a much thicker one (the existing \"page number\" dimension grew).\n",
        "\n",
        "#### Practical Comparison using your `x` tensor (Shape 2, 3)\n",
        "\n",
        "**1. Using `torch.stack((x, x), dim=0)`**\n",
        "\n",
        "* Inputs: Shape (2,3) and (2,3).\n",
        "* Operation: Create NEW dimension 0.\n",
        "* Result Shape: **(2, 2, 3)**. It's now a 3D tensor.\n",
        "\n",
        "**2. Using `torch.cat((x, x), dim=0)`**\n",
        "\n",
        "* Inputs: Shape (2,3) and (2,3).\n",
        "* Operation: Join along EXISTING dimension 0 (rows).\n",
        "* We have 2 rows, and we add another 2 rows below them.\n",
        "* Result Shape: **(4, 3)**. It stays a 2D tensor, just taller.\n",
        "\n",
        "```python\n",
        "# Result of torch.cat((x, x), dim=0)\n",
        "tensor([[ 0.3367,  0.1288,  0.2345], # x row 0\n",
        "        [ 0.2303, -1.1229, -0.1863], # x row 1\n",
        "        [ 0.3367,  0.1288,  0.2345], # x row 0 appended\n",
        "        [ 0.2303, -1.1229, -0.1863]])# x row 1 appended\n",
        "\n",
        "```\n",
        "\n",
        "**3. Using `torch.cat((x, x), dim=1)`**\n",
        "\n",
        "* Inputs: Shape (2,3) and (2,3).\n",
        "* Operation: Join along EXISTING dimension 1 (columns).\n",
        "* We have 3 columns, and we attach another 3 columns to the right side.\n",
        "* Result Shape: **(2, 6)**. It stays a 2D tensor, just wider.\n",
        "\n",
        "```python\n",
        "# Result of torch.cat((x, x), dim=1)\n",
        "# Row 0 of x joined with Row 0 of x\n",
        "tensor([[ 0.3367,  0.1288,  0.2345,  0.3367,  0.1288,  0.2345],\n",
        "# Row 1 of x joined with Row 1 of x\n",
        "        [ 0.2303, -1.1229, -0.1863,  0.2303, -1.1229, -0.1863]])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ZVyYbQ0q2aS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)\n",
        "t2 = torch.stack([tensor, tensor, tensor], dim=1)\n",
        "print(t2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZfDvU6T2aCI",
        "outputId": "010330e4-fb39-4c0a-9ed3-6d2be619057c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
            "tensor([[[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Multiplying tensors"
      ],
      "metadata": {
        "id": "FQyzRD6A30Sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This computes the element-wise product\n",
        "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor * tensor \\n {tensor * tensor}\")\n",
        "\n",
        "# This computes the matrix multiplication between two tensors\n",
        "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPe_W4xj3o-x",
        "outputId": "8f261c2a-070e-4d5c-d7f4-e18a0f371c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor.mul(tensor) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor * tensor \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor.matmul(tensor.T) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            "\n",
            "tensor @ tensor.T \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In-place operations\n",
        "Operations that have a `_` suffix are in-place. For example: `x.copy_(y)`, `x.t_()`, will change `x`."
      ],
      "metadata": {
        "id": "b4bioOnQ4RCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor, \"\\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrnFqXNt4an5",
        "outputId": "b8468e22-836b-4921-8211-e2618978a7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch.dtype\n",
        "\n",
        "A torch.dtype is an object that represents the data type of a torch.Tensor. PyTorch has several different data types:\n",
        "\n",
        "### Floating point dtypes\n",
        "\n",
        "| dtype | description |\n",
        "| :---- | :---------- |\n",
        "| `torch.float32` or `torch.float` | 32-bit floating point |\n",
        "| `torch.float64` or `torch.double` | 64-bit floating point |\n",
        "| `torch.float16` or `torch.half` | 16-bit floating point, S-E-M 1-5-10 |\n",
        "| `torch.bfloat16` | 16-bit floating point, sometimes referred to as Brain floating point, S-E-M 1-8-7 |\n",
        "| `torch.complex32` or `torch.chalf` | 32-bit complex with two float16 components |\n",
        "| `torch.complex64` or `torch.cfloat` | 64-bit complex with two float32 components |\n",
        "| `torch.complex128` or `torch.cdouble` | 128-bit complex with two float64 components |\n",
        "| `torch.float8_e4m3fn` | 8-bit floating point, S-E-M 1-4-3|\n",
        "| `torch.float8_e5m2` [shell] | 8-bit floating point, S-E-M 1-5-2|\n",
        "| `torch.float8_e4m3fnuz` | 8-bit floating point, S-E-M 1-4-3|\n",
        "| `torch.float8_e5m2fnuz` | 8-bit floating point, S-E-M 1-5-2|\n",
        "| `torch.float8_e8m0fnu` | 8-bit floating point, S-E-M 0-8-0|\n",
        "| `torch.float4_e2m1fn_x2` | packed 4-bit floating point, S-E-M 1-2-1|\n",
        "\n",
        "### Integer dtypes\n",
        "\n",
        "| dtype | description |\n",
        "| :---- | :---------- |\n",
        "| `torch.uint8` | 8-bit integer (unsigned) |\n",
        "| `torch.int8` | 8-bit integer (signed) |\n",
        "| `torch.uint16` | 16-bit integer (unsigned) |\n",
        "| `torch.int16` or `torch.short` | 16-bit integer (signed) |\n",
        "| `torch.uint32` | 32-bit integer (unsigned) |\n",
        "| `torch.int32` or `torch.int` | 32-bit integer (signed) |\n",
        "| `torch.uint64` | 64-bit integer (unsigned) |\n",
        "| `torch.int64` or `torch.long` | 64-bit integer (signed) |\n",
        "| `torch.bool` | Boolean |"
      ],
      "metadata": {
        "id": "_z0C-oXU5smA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-z9T1VXbHW5m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "98dIfyzm5tvs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}